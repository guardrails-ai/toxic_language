# Overview

| Developed by | Guardrails AI |
| --- | --- |
| Date of development | Feb 15, 2024 |
| Validator type | Moderation |
| Blog | - |
| License | Apache 2 |
| Input/Output | Output |

# Description

This validator checks any text for the presence of toxicity in the LLM output. Under the hood, it invokes this open model on the Huggingface repo and 

## Intended use

- Primary intended uses:
- Out-of-scope use cases:

## Resources required

- Dependencies:
- Foundation model access keys:
- Compute:

## Expected deployment metrics

|  | CPU | GPU |
| --- | --- | --- |
| Latency | - | - |
| Memory | - | - |
| Cost | - | - |
| Expected quality | - | - |

## Validator Performance

### Evaluation Dataset

N/A

### Model Performance Measures

N/A

### Decision thresholds

N/A

# Installation

# Usage Examples

## Validating string output via Python

In this example, we apply the validator to a string output generated by an LLM.

```python
# Import Guard and Validator
from guardrails.hub import BugFreePython
from guardrails import Guard

# Initialize Validator
val = BugFreePython(on_fail="fix")

# Setup Guard
guard = Guard.from_string(
    validators=[val, ...],
)

# Correct python
correct_python = """
import os

def foo():
		print(f"Current path is: {os.getcwd()}")

foo()
"""

incorrect_python = """
import os

def foo()
		print f"Current path is: {os.getcwd()}"

foo()
"""

guard.parse(correct_python)  # Validator passes
guard.parse(incorrect_python)  # Validator fails
```

## Validating JSON output via Python

In this example, we apply the validator on the string field of a JSON output.

```python
# Import Guard and Validator
from pydantic import BaseModel
from guardrails.hub import BugFreePython
from guardrails import Guard

val = BugFreePython(on_fail="fix")

# Create Pydantic BaseModel
class ProgramGen(BaseModel):
		program_description: str
		code: str = Field(
				description="Generated code", validators=[val]
		)

# Create a Guard to check for valid Pydantic output
guard = Guard.from_pydantic(output_class=ProgramGen)

# Run LLM output generating JSON through guard
guard.parse("""
{
		"program_description": "Caesar",
		"code": "
import os

def foo():
		print(f"Current path is: {os.getcwd()}")

foo()
"
}
""")
```

## Validating string output via RAIL

tbd

## Validating JSON output via RAIL

tbd

# API Reference

`__init__`

- 
- `on_fail`: The policy to enact when a validator fails.
